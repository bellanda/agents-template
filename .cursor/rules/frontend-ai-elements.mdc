---
description: AI Elements (Vercel) - Components for AI-native interfaces with LangGraph backend
globs: ["frontend/**/*.tsx", "frontend/**/*.ts"]
alwaysApply: false
---

# AI Elements - Vercel AI SDK Components

## Rule #1: AI Elements First for AI Features

**ALWAYS check if an AI Elements component exists before creating custom AI UI components.**

1. SEARCH this list for AI-related components
2. USE import from `@/components/ai-elements/...`
3. COMPOSE with Shadcn for complex UIs
4. CUSTOM only if IMPOSSIBLE with AI Elements

## Documentation Links

- [Message](https://elements.ai-sdk.dev/components/message) - Message, MessageContent, MessageResponse
- [Reasoning](https://elements.ai-sdk.dev/components/reasoning) - Reasoning, ReasoningTrigger, ReasoningContent
- [Conversation](https://elements.ai-sdk.dev/components/conversation)
- [Prompt Input](https://elements.ai-sdk.dev/components/prompt-input)
- [Shimmer](https://elements.ai-sdk.dev/components/shimmer)
- [Chain of Thought](https://elements.ai-sdk.dev/components/chain-of-thought)

## Prerequisites

- `bun add ai @ai-sdk/react`
- shadcn/ui + Tailwind CSS in CSS Variables mode
- Backend must use Vercel AI SDK Data Stream Protocol (see backend-ai-fastapi-langchain rule)

## globals.css - Required for MessageResponse

```css
@source "../node_modules/streamdown/dist/*.js";
```

## Component Reference

### Conversation
- **Conversation** - Container with auto-scroll
- **ConversationContent** - Message list wrapper
- **ConversationEmptyState** - Empty state
- **ConversationScrollButton** - Scroll-to-bottom button

### Message
- **Message** - `from={message.role}` (user/assistant)
- **MessageContent** - Content wrapper
- **MessageResponse** - Markdown rendering (uses Streamdown); use for `part.type === "text"`

### Reasoning
- **Reasoning** - Collapsible, auto-open during streaming, auto-close when done
- **ReasoningTrigger** - Shows "Thinking..." shimmer when streaming
- **ReasoningContent** - `children: string` for reasoning text

### Input
- **PromptInput**, **PromptInputTextarea**, **PromptInputSubmit** - Chat input

## Message Parts Rendering (LangGraph Backend)

When backend sends reasoning + text (Data Stream Protocol), render parts in order:

```tsx
const sorted = [...(message.parts ?? [])].sort((a, b) => {
  const order: Record<string, number> = { reasoning: 0, text: 1 };
  return (order[a?.type] ?? 2) - (order[b?.type] ?? 2);
});

{sorted.map((part, i) => {
  if (!part || typeof part !== "object") return null;
  const textContent = "text" in part ? String((part as { text?: string }).text ?? "") : "";
  switch (part.type) {
    case "text":
      return <MessageResponse key={`${message.id}-${i}`}>{textContent}</MessageResponse>;
    case "reasoning":
      return (
        <Reasoning
          key={`${message.id}-${i}`}
          isStreaming={chatStatus === "streaming" && isLastMessage && !hasTextPart}
          open={reasoningOpenMap[message.id] ?? (hasTextPart ? false : true)}
          onOpenChange={(open) => setReasoningOpenMap((p) => ({ ...p, [message.id]: open }))}
        >
          <ReasoningTrigger />
          <ReasoningContent>{textContent}</ReasoningContent>
        </Reasoning>
      );
    default:
      return null;
  }
})}
```

## Reasoning Control

- **isStreaming**: true only when chat is streaming AND no text part yet (reasoning phase)
- **open**: Controlled. When `hasTextPart`, pass `false` so reasoning collapses when response arrives.
- **onOpenChange**: Store in state (e.g. `reasoningOpenMap`) so user can re-open after collapse.

## Long Reasoning - Scroll + Hide Scrollbar

Wrap ReasoningContent in a div with auto-scroll and hidden scrollbar:

```tsx
<div
  ref={scrollRef}
  className="max-h-48 overflow-y-auto [scrollbar-width:none] [-ms-overflow-style:none] [&::-webkit-scrollbar]:hidden"
>
  <ReasoningContent>{textContent}</ReasoningContent>
</div>
```

Use `useLayoutEffect` to set `scrollTop = scrollHeight` when content changes during streaming.

## Persistence - Extract Text for Backend Save

- **extractMessageText**: Joins `text` + `reasoning` (for display/fallback)
- **extractMessageResponseText**: Only `text` parts (for saving - reasoning is internal, response is deliverable)

```tsx
const extractMessageResponseText = (msg: UIMessage): string => {
  if (!Array.isArray(msg.parts)) return (msg as { content?: string }).content ?? "";
  return msg.parts
    .filter((p) => p?.type === "text")
    .map((p) => String((p as { text?: string }).text ?? ""))
    .filter(Boolean)
    .join(" ")
    .trim() || (msg as { content?: string }).content ?? "";
};
```

Use `extractMessageResponseText` when calling `onAgentAnalysisChange` or persisting to backend.

## useChat / useAgentChat

```tsx
const { messages, setMessages, sendMessage, status, error, stop } = useChat({
  transport: new DefaultChatTransport({
    api: `${API_URL}/agents/chat/completions`,
    body: { model: "my-agent", stream: true },
  }),
});

const isChatStreaming = status === "submitted" || status === "streaming";
```

- **setMessages**: Use to clear messages for single-shot regenerate
- **status**: `"ready"` | `"submitted"` | `"streaming"`

## Single-Shot vs Conversation

- **Single-shot** (e.g. analysis): On regenerate: `setMessages([])`, reset `lastStoredMessageIdRef`, `setReasoningOpenMap({})`, then `sendMessage`
- **Conversation**: Keep messages, append

## Persistence Flow

```tsx
useEffect(() => {
  const lastAssistant = [...messages].reverse().find((m) => m.role === "assistant");
  if (!lastAssistant || isChatStreaming || lastStoredMessageIdRef.current === lastAssistant.id) return;

  const text = extractMessageResponseText(lastAssistant);
  lastStoredMessageIdRef.current = lastAssistant.id;
  onAgentAnalysisChange?.(text);
}, [messages, isChatStreaming, onAgentAnalysisChange]);
```

Parent route must call backend (e.g. `updateAssessment`) from `onAgentAnalysisChange` for immediate persistence.

## Important Rules

1. Use **MessageResponse** for AI text (markdown via Streamdown), not a generic Response
2. Sort parts: reasoning before text
3. Guard `part` before access: `if (!part || typeof part !== "object") return null`
4. Use `extractMessageResponseText` for persistence (only text part)
5. Backend must use Vercel AI SDK Data Stream Protocol (see backend-ai-fastapi-langchain)
