---
description: FastAPI + LangGraph Agents - Backend streaming, frontend integration, and full-stack patterns
globs: agents/**/*.py, api/**/*.py, config/*.py, frontend/src/**/*.{ts,tsx}
---

# FastAPI + LangGraph Agents - Full-Stack Pattern

## Architecture Overview

This project implements a multi-agent backend using:
- **LangGraph** (`create_react_agent`) - Core orchestration framework
- **Auto-discovery** - Agents loaded from `agents/` directory
- **Vercel AI SDK Data Stream Protocol** - SSE streaming compatible with `useChat` and AI Elements
- **Reasoning Support** - `ChatChutes` captures reasoning from Chutes/DeepSeek providers

```mermaid
flowchart LR
    subgraph Frontend
        useChat[useChat/useAgentChat]
        Transport[DefaultChatTransport]
        useChat --> Transport
    end
    subgraph Backend
        ChatAPI[POST /chat/completions]
        Extract[_extract_user_query]
        Stream[stream_agent]
        LangGraph[astream_events]
    end
    Transport -->|"messages, body"| ChatAPI
    ChatAPI --> Extract
    Extract --> Stream
    Stream --> LangGraph
    LangGraph -->|"on_chat_model_stream"| Stream
    Stream -->|"SSE chunks"| Transport
```

## Directory Structure

```
.
├── agents/                     # Agent definitions (auto-discovered)
│   ├── web_search_agent/
│   │   ├── agent.py            # Must export root_agent
│   │   └── tools/
│   └── your_agent/
│       ├── agent.py
│       └── tools/
├── api/
│   ├── routes/agents/
│   │   ├── chat.py             # OpenAI-compatible endpoint
│   │   └── models.py
│   └── services/agents/
│       ├── registry.py
│       ├── executors.py
│       ├── streaming.py
│       └── tools.py
└── config/
    ├── api_keys.py
    └── agents.py               # init_chutes_model
```

## Backend - Model and Agent Patterns

### Model Initialization
ALWAYS use `init_chutes_model` from `config.agents` for reasoning capture.

```python
from config.agents import init_chutes_model

model = init_chutes_model(model="zai-org/GLM-4.7-TEE", streaming=True, temperature=0.3)
```

### Agent Definition
```python
# agents/your_agent/agent.py
from config.agents import init_chutes_model
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.prebuilt import create_react_agent

model = init_chutes_model(model="zai-org/GLM-4.7-TEE")
tools = [your_tool]
checkpointer = InMemorySaver()
root_agent = create_react_agent(model=model, tools=tools, prompt="...", checkpointer=checkpointer)

AGENT_NAME = "My Agent"
AGENT_DESCRIPTION = "Description for UI"
```

### Tool Definition
```python
from langchain.tools import tool
from pydantic import BaseModel, Field

class ToolInput(BaseModel):
    query: str = Field(description="Search query")

@tool("my_tool", args_schema=ToolInput)
def my_tool(query: str) -> str:
    """When to use this tool."""
    return f"Result for {query}"
```

## Backend - Streaming Protocol (Critical)

### Required Header
```python
headers={"x-vercel-ai-ui-message-stream": "v1"}
```

### Chunk Lifecycle (Reasoning MUST come before text)
```python
# api/services/agents/streaming.py
yield f"data: {orjson.dumps({'type': 'start', 'messageId': completion_id}).decode('utf-8')}\n\n"

# If reasoning exists: reasoning-start, reasoning-delta*, reasoning-end
# Then: text-start, text-delta*, text-end
yield f"data: {orjson.dumps({'type': 'finish'}).decode('utf-8')}\n\n"
yield "data: [DONE]\n\n"
```

### Chunk Helpers
```python
def _chunk(type_name: str, id: str, delta: str = "") -> str:
    payload = {"type": type_name, "id": id}
    if delta:
        payload["delta"] = delta
    return f"data: {orjson.dumps(payload).decode('utf-8')}\n\n"

def _error_chunk(error_text: str) -> str:
    payload = {"type": "error", "errorText": error_text}
    return f"data: {orjson.dumps(payload).decode('utf-8')}\n\n"
```

### Rules
- Use `.decode('utf-8')` on `orjson.dumps()` - never use `OPT_INDENT_2`
- Error type uses `errorText`, not `delta`
- Send `text-start` only when first text chunk arrives (after reasoning if any)

### _extract_user_query (chat.py)
Supports: `parts` (Vercel SDK), `content` string, `content` array. Always use last user message.

```python
# Vercel AI SDK format
parts = last_msg.get("parts")
if isinstance(parts, list):
    for part in parts:
        if part.get("type") == "text":
            texts.append(part.get("text", ""))
```

## Frontend - AI Elements Integration

### Component References
- [Message](https://elements.ai-sdk.dev/components/message) - Message, MessageContent, MessageResponse
- [Reasoning](https://elements.ai-sdk.dev/components/reasoning) - Reasoning, ReasoningTrigger, ReasoningContent
- [Conversation](https://elements.ai-sdk.dev/components/conversation) - Conversation, ConversationContent, ConversationEmptyState, ConversationScrollButton
- [Shimmer](https://elements.ai-sdk.dev/components/shimmer) - Loading indicator
- [Prompt Input](https://elements.ai-sdk.dev/components/prompt-input) - Chat input

### globals.css Required
```css
@source "../node_modules/streamdown/dist/*.js";
```

### useChat / useAgentChat
- Transport: `DefaultChatTransport` with `api: /api/agents/chat/completions`
- Returns: `messages`, `sendMessage`, `setMessages`, `status`, `error`, `stop`
- `status`: `"submitted"` | `"streaming"` | `"ready"`

### Message Parts Rendering (Canonical Pattern)
```tsx
{message.parts?.map((part, i) => {
  if (!part || typeof part !== "object") return null;
  const textContent = "text" in part ? String((part as { text?: string }).text ?? "") : "";
  switch (part.type) {
    case "text":
      return <MessageResponse key={`${message.id}-${i}`}>{textContent}</MessageResponse>;
    case "reasoning":
      return (
        <Reasoning key={`${message.id}-${i}`} isStreaming={...} open={...} onOpenChange={...}>
          <ReasoningTrigger />
          <ReasoningContent>{textContent}</ReasoningContent>
        </Reasoning>
      );
    default:
      return null;
  }
})}
```

### Part Order (Reasoning before Text)
Sort parts so reasoning renders above response:
```tsx
const sorted = [...parts].sort((a, b) => {
  const order: Record<string, number> = { reasoning: 0, text: 1 };
  return (order[a?.type] ?? 2) - (order[b?.type] ?? 2);
});
```

### Reasoning Component Control
- `isStreaming`: `chatStatus === "streaming" && isLastMessage && !hasTextPart`
- `open` / `onOpenChange`: Use `reasoningOpenMap` state. When `hasTextPart`, pass `open={false}` so reasoning collapses; user can re-open via click.
- `defaultOpen`: true

### Long Reasoning - Scroll Wrapper
```tsx
function ReasoningScrollWrapper({ children, content, isStreaming }) {
  const scrollRef = useRef<HTMLDivElement>(null);
  useLayoutEffect(() => {
    if (isStreaming && scrollRef.current) {
      scrollRef.current.scrollTop = scrollRef.current.scrollHeight;
    }
  }, [content, isStreaming]);
  return (
    <div
      ref={scrollRef}
      className="max-h-48 overflow-y-auto [scrollbar-width:none] [-ms-overflow-style:none] [&::-webkit-scrollbar]:hidden"
    >
      {children}
    </div>
  );
}
```

### Persistence - extractMessageText vs extractMessageResponseText
- `extractMessageText`: Joins `text` + `reasoning` (display/fallback)
- `extractMessageResponseText`: Only `text` parts (for saving - reasoning is internal)

```tsx
const extractMessageResponseText = (msg: UIMessage): string => {
  if (!Array.isArray(msg.parts)) return (msg as { content?: string }).content ?? "";
  return msg.parts
    .filter((p) => p?.type === "text")
    .map((p) => String((p as { text?: string }).text ?? ""))
    .filter(Boolean)
    .join(" ")
    .trim() || (msg as { content?: string }).content ?? "";
};
```

### Persistence Flow
When stream finishes: `useEffect` on `[messages, isChatStreaming]` -> when `!isChatStreaming` and last assistant exists and not already stored -> `extractMessageResponseText(lastAssistant)` -> `onAgentAnalysisChange?.(text)`.

For immediate backend save, parent route must call `updateAssessment` (or equivalent) from `onAgentAnalysisChange`; otherwise data is only in local state until form submit.

### Single-Shot vs Conversation
- **Single-shot** (e.g. analysis): On regenerate: `setMessages([])`, reset refs, `setReasoningOpenMap({})`, then `sendMessage`
- **Conversation**: Keep messages, append

## Checklist - Add Agent Chat to a New Route

1. Use `useAgentChat` or `useChat` with transport to `/api/agents/chat/completions`
2. Render messages: loop `message.parts`, handle `text` and `reasoning`; sort reasoning first
3. Reasoning: correct `isStreaming`, `open`/`onOpenChange` for close-when-done + manual reopen
4. If persisting: `useEffect` when `!isChatStreaming`, use `extractMessageResponseText`, call parent callback
5. Single-shot: clear messages before `sendMessage` on regenerate

## Best Practices

1. **Backend**: Always `orjson`, no `OPT_INDENT_2`, type hints on public functions
2. **Frontend**: Guard `part` before access; use `String()` for safe text
3. **Streaming**: Reasoning before text; `start` and `finish` and `[DONE]` required
