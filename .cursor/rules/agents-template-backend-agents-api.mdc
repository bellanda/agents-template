---
description: "[Agents Template] Backend - agents (factory, modes), API routes, streaming protocol, persistence, threads"
globs: agents/**/*.py, api/**/*.py, config/*.py
alwaysApply: false
---

# Agents Template – Backend (agents, API, streaming, persistence)

Rules for the **agents-template** backend: agent definitions, streaming protocol, checkpointer, threads API. Use only when working on backend/agents in a project that uses this template.

## 1. Agent Definition (agents/*/agent.py)

Each agent is auto-discovered. **Required exports:**

```python
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.base import BaseCheckpointSaver

AGENT_NAME = "Human-readable Name"
AGENT_DESCRIPTION = "What this agent does"
AGENT_MODE = "chat"  # or "single-shot"

def create_root_agent(checkpointer: BaseCheckpointSaver | None = None):
    return create_react_agent(
        model=model,
        tools=tools,
        prompt=SYSTEM_PROMPT,
        checkpointer=checkpointer,
    )
```

- **chat**: registry passes shared `AsyncSqliteSaver`; conversations persist.
- **single-shot**: registry passes `None`; each request is independent.
- Folder name → model ID: `weather_agent` → `weather-agent`.

**Models:** Chutes/reasoning → `from config.agents import init_chutes_model`. OpenAI-compatible → `init_chat_model(..., streaming=True)`.

**Tools:** `@tool("name", args_schema=PydanticModel)` in `agents/<name>/tools/`.

---

## 2. Streaming Protocol (api/services/agents/streaming.py)

SSE compatible with AI SDK `DefaultChatTransport`. **Chunk order:**

`start` → [reasoning-start → reasoning-delta* → reasoning-end] → text-start → text-delta* → text-end → finish → `data: [DONE]\n\n`

**Helpers:**

```python
def _chunk(type_name: str, id: str, delta: str = "") -> str:
    payload: dict = {"type": type_name, "id": id}
    if delta:
        payload["delta"] = delta
    return f"data: {orjson.dumps(payload).decode('utf-8')}\n\n"

def _error_chunk(error_text: str) -> str:
    return f"data: {orjson.dumps({'type': 'error', 'errorText': error_text}).decode('utf-8')}\n\n"
```

**Rules:** Use `orjson.dumps().decode('utf-8')`; no extra fields (strictObject). `start` uses `messageId` not `id`. Response header: `x-vercel-ai-ui-message-stream: v1`. Reasoning from `chunk.additional_kwargs.get("reasoning_content", "")`.

---

## 3. Persistence & Threads

**Checkpointer:** `config/checkpointer.py` — `aiosqlite.connect(DB_PATH)` + `AsyncSqliteSaver(conn)`; `await conn.setup()`. Lifespan in `main.py`: `init_checkpointer()` → `reload_agents_registry()` → yield → `close_checkpointer()`.

**Threads API:** `GET /agents/threads` (optional `?agent_id=`), `GET /agents/threads/{id}`, `DELETE /agents/threads/{id}`. List uses `checkpointer.alist({})`; guard `config`/`configurable` with `getattr` and `isinstance(config, dict)` to avoid KeyError.

**Prompt cache:** `config/prompt_cache.py` — `should_apply_prompt_cache(model)` and `apply_anthropic_cache_control(messages)` for Anthropic only; OpenAI is automatic; Groq/Chutes/NVIDIA per-request.
