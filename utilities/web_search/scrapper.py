import re
import time

import requests
from bs4 import BeautifulSoup

from api.utils.tools import generate_error_message, generate_result_message


def scrape_url(url: str) -> str:
    """Scrape content from a single URL."""
    start_time = time.time()

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")

        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Get text content
        text = soup.get_text()

        # Clean up text
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = " ".join(chunk for chunk in chunks if chunk)

        end_time = time.time()
        success_msg = generate_result_message("success", f"Scraped {url} in {end_time - start_time:.2f}s")
        print(f"üîç {success_msg}")

        return text

    except Exception as e:
        end_time = time.time()
        error_msg = generate_error_message(f"Failed to scrape {url} in {end_time - start_time:.2f}s: {str(e)}")
        print(f"‚ùå {error_msg}")
        return ""


def perform_scraping(url: str) -> str:
    """Legacy function for backward compatibility."""
    start_time = time.time()

    try:
        content = scrape_url(url)
        if content:
            return content
        else:
            return f"ERRO_SCRAPING: N√£o foi poss√≠vel extrair conte√∫do de {url}"

    except Exception as e:
        end_time = time.time()
        error_msg = generate_error_message(f"Error parsing {url} in {end_time - start_time:.2f}s: {str(e)}")
        print(f"‚ùå {error_msg}")
        return f"ERRO_EXCEPTION: {str(e)}"


def smart_strip(text: str) -> str:
    """
    Remove espa√ßos e quebras de linha excessivos, mantendo formata√ß√£o leg√≠vel.
    - Reduz m√∫ltiplos espa√ßos para no m√°ximo 2
    - Reduz m√∫ltiplas quebras de linha para no m√°ximo 3
    - Remove espa√ßos no in√≠cio e fim de cada linha
    """
    # Remove espa√ßos no in√≠cio e fim
    text = text.strip()

    # Substitui m√∫ltiplos espa√ßos (6 ou mais) por 2 espa√ßos
    text = re.sub(r" {6,}", "  ", text)

    # Substitui m√∫ltiplos espa√ßos (3-5) por 1 espa√ßo
    text = re.sub(r" {3,5}", " ", text)

    # Remove espa√ßos no in√≠cio e fim de cada linha
    lines = text.split("\n")
    lines = [line.strip() for line in lines]
    text = "\n".join(lines)

    # Substitui m√∫ltiplas quebras de linha (5 ou mais) por 3
    text = re.sub(r"\n{5,}", "\n\n\n", text)

    # Substitui m√∫ltiplas quebras de linha (4) por 2
    text = re.sub(r"\n{4}", "\n\n", text)

    return text


if __name__ == "__main__":
    import pathlib
    import sys

    sys.path.append("/home/bellanda/code/projects-templates/agents-template")

    current_dir = pathlib.Path(__file__).parent

    from llms.groq import call_groq_llm

    result = perform_scraping("https://bellanda.github.io/dev/")
    with open(current_dir / "result.txt", "w", encoding="utf-8") as f:
        f.write(result)

    prompt = f"""
        Voc√™ √© um analista de conte√∫dos da web especializado em condensar grandes volumes de texto.

        ## Objetivo
        Receber **exatamente 5** p√°ginas completas ‚Äî cada uma no formato:

        ### <T√≠tulo da P√°gina N>
        <corpo completo da P√°gina N>

        e devolver **uma resposta √∫nica** que:
        1. ConteÃÇm todas as informacÃßoÃÉes essenciais (fatos, nuÃÅmeros, datas, nomes, conclusoÃÉes).
        2. Elimina repeticÃßoÃÉes e detalhes triviais (ex.: c√≥digo, an√∫ncios, bot√µes).
        3. Resolva poss√≠veis contradi√ß√µes ou indique diverg√™ncias, se houver.
        4. J√° esteja pronta para ser lida por um agente-chamador (n√£o precisa de p√≥s-processamento).

        ## Passos internos (Chain-of-Thought oculto)
        1. **Para cada p√°gina**: extraia
        ‚Ä¢ T√≥picos centrais  
        ‚Ä¢ Dados quantitativos e cita√ß√µes relevantes  
        ‚Ä¢ Conclus√£o ou take-away principal
        2. **Agrupe** t√≥picos semelhantes das cinco p√°ginas e integre informa√ß√µes complementares.
        3. **Detecte incoer√™ncias**: se dois textos divergem, aponte brevemente (‚ÄúFonte A diz X, Fonte B diz Y‚Äù).
        4. **Monte o resumo final** (veja formato abaixo).

        ## Formato de sa√≠da (exporte apenas o texto abaixo)
        **Resumo Integrado (‚â§ 250 palavras)**  
        Um par√°grafo ou lista curta que responda de forma direta ao tema comum das p√°ginas.

        **Principais Pontos Organizados**
        - *T√≥pico 1*: frase-s√≠ntese + dados / exemplos
        - *T√≥pico 2*: ‚Ä¶
        *(Use no m√°x. 7 bullets.)*

        **Notas de Diverg√™ncia (opcional)**
        - Se houver conflito: ‚ÄúP√°gina 1 vs P√°gina 3: ‚Ä¶‚Äù

        **Mapa de Origem**
        T√≠tulo da P√°gina 1 ‚Üí [ideia-chave-1, ideia-chave-2]  
        T√≠tulo da P√°gina 2 ‚Üí ‚Ä¶  
        *(Cite apenas as ideias condensadas ‚Äî n√£o copie texto literal.)*

        ## Regras
        - Escreva em **portugu√™s claro e conciso**.
        - N√£o invente informa√ß√µes; baseie-se apenas no texto recebido.
        - N√£o inclua explica√ß√µes sobre seu racioc√≠nio.
        - N√£o formate como c√≥digo nem rode markdown excessivo (use headings e bullets simples).
        - Se faltar uma das 5 p√°ginas, mencione: ‚ÄúAviso: p√°gina X ausente‚Äù.

        (Conte√∫do fornecido come√ßa abaixo.)
        {result}
    """

    with open(current_dir / "result_summarized.txt", "w", encoding="utf-8") as f:
        f.write(call_groq_llm("llama3-8b-8192", prompt, max_tokens=1024))
