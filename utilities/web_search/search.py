import pathlib
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.append(str(pathlib.Path(__file__).parent.parent.parent))

# from llms.nvidia import call_nvidia_llm
from llms.google import call_google_llm
from utilities.web_search.duckduckgo import search as search_duckduckgo
from utilities.web_search.scrapper import perform_scraping

BASE_DIR = pathlib.Path(__file__).parent.parent.parent


def scrape_single_result(result_with_index):
    """Fun√ß√£o auxiliar para fazer scraping de uma √∫nica p√°gina"""
    i, result = result_with_index
    content = perform_scraping(result["url"])
    return i, result, content


def search(query: str) -> str:
    print(f"üîç [SEARCH] Iniciando busca para: '{query}'")
    results = search_duckduckgo(query)
    print(f"üìä [SEARCH] Encontrados {len(results)} resultados do DuckDuckGo")

    full_content = ""
    successful_scrapes = 0
    failed_scrapes = 0

    # Usar ThreadPoolExecutor para fazer scraping em paralelo
    print(f"üöÄ [SEARCH] Iniciando scraping paralelo de {len(results)} p√°ginas...")

    # Preparar dados para o ThreadPoolExecutor
    results_with_index = [(i, result) for i, result in enumerate(results)]
    scraped_results = {}

    # Executar scraping em paralelo
    with ThreadPoolExecutor(max_workers=min(len(results), 10)) as executor:
        # Submeter todas as tarefas
        future_to_index = {
            executor.submit(scrape_single_result, result_with_index): result_with_index[0]
            for result_with_index in results_with_index
        }

        # Processar resultados conforme completam
        for future in as_completed(future_to_index):
            try:
                i, result, content = future.result()
                scraped_results[i] = (result, content)

                # Verifica se o scraping foi bem-sucedido
                if content.startswith("ERRO_"):
                    failed_scrapes += 1
                    print(f"‚ö†Ô∏è [SEARCH] P√°gina {i + 1} falhou: {result['url']}")
                else:
                    successful_scrapes += 1
                    print(f"‚úÖ [SEARCH] P√°gina {i + 1} processada: {result['url']}")

            except Exception as e:
                i = future_to_index[future]
                failed_scrapes += 1
                print(f"‚ùå [SEARCH] Erro na p√°gina {i + 1}: {str(e)}")
                # Adicionar resultado com erro
                scraped_results[i] = (results[i], f"ERRO_EXCEPTION: {str(e)}")

    # Montar o conte√∫do final na ordem original
    for i in range(len(results)):
        if i in scraped_results:
            result, content = scraped_results[i]
            full_content += f"P√°gina {i + 1}\nURL: {result['url']}\nT√≠tulo: {result['title']}\nDescri√ß√£o: {result['description']}\n\n{content}\n\n"

    print(f"üìà [SEARCH] Resumo: {successful_scrapes} sucessos, {failed_scrapes} falhas de {len(results)} p√°ginas")

    prompt = f"""
        Voc√™ √© um analista de conte√∫dos da web especializado em condensar grandes volumes de texto.

        ## Objetivo
        Receber **exatamente 5** p√°ginas completas ‚Äî cada uma no formato:

        ### <T√≠tulo da P√°gina N>
        <corpo completo da P√°gina N>

        e devolver **uma resposta √∫nica em Markdown** que:
        1. Cont√©m todas as informa√ß√µes essenciais (fatos, n√∫meros, datas, nomes, conclus√µes).
        2. Elimina repeti√ß√µes e detalhes triviais (ex.: c√≥digo-fonte, an√∫ncios, bot√µes).
        3. Resolve poss√≠veis contradi√ß√µes ou indica diverg√™ncias, se houver.
        4. J√° esteja pronta para ser exibida no front-end (n√£o precisa de p√≥s-processamento).

        ## Passos internos (Chain-of-Thought oculto)  
        1. **Para cada p√°gina**: extraia  
        ‚Ä¢ T√≥picos centrais  
        ‚Ä¢ Dados quantitativos e cita√ß√µes relevantes  
        ‚Ä¢ Conclus√£o ou insight principal  
        2. **Agrupe** t√≥picos semelhantes das cinco p√°ginas e integre informa√ß√µes complementares.  
        3. **Detecte incoer√™ncias**: se dois textos divergem, aponte brevemente (‚ÄúFonte A diz X, Fonte B diz Y‚Äù).  
        4. **Monte o resumo final** (veja formato abaixo).

        ## Formato de sa√≠da (exporte **apenas** o texto abaixo)  
        > Use **Markdown**, cabe√ßalhos, bullets e **emojis** üëÄ para tornar a visualiza√ß√£o agrad√°vel.  
        > Inclua o **link da URL** de cada p√°gina onde indicado.

        **Resumo Integrado**  
        (Escreva de forma clara e concisa; escolha um tamanho que cubra todos os pontos essenciais sem se alongar demais.)

        **Principais Pontos Organizados**  
        - üîπ *T√≥pico 1*: frase-s√≠ntese + dados / exemplos  
        - üîπ *T√≥pico 2*: ‚Ä¶  
        *(M√°x. 7 bullets.)*

        **Notas de Diverg√™ncia ‚ö†Ô∏è (opcional)**  
        - Se houver conflito: ‚ÄúP√°gina 1 vs P√°gina 3: ‚Ä¶‚Äù

        **Mapa de Origem**  
        [üîó P√°gina 1 ‚Äì T√≠tulo](URL-1) ‚Üí ideia-chave-1, ideia-chave-2  
        [üîó P√°gina 2 ‚Äì T√≠tulo](URL-2) ‚Üí ‚Ä¶  
        *(Liste apenas as ideias condensadas ‚Äî n√£o copie texto literal.)*

        ## Regras
        - Escreva em **portugu√™s** claro.
        - N√£o invente informa√ß√µes; baseie-se somente no texto das p√°ginas.
        - Se faltar uma das 5 p√°ginas, mencione: ‚ÄúAviso: p√°gina X ausente‚Äù.  
        - Evite blocos de c√≥digo; use formata√ß√£o simples de Markdown.
        
        CONTE√öDO:
        ```text
        {full_content}
        ```
    """
    # Create scrapper directory
    SCRAPPER_DIR = BASE_DIR / "data/scrapper"
    SCRAPPER_DIR.mkdir(parents=True, exist_ok=True)

    # Save raw web search results
    RAW_WEB_SEARCH_RESULTS_FILE = SCRAPPER_DIR / "raw_web_search_results.txt"
    with open(RAW_WEB_SEARCH_RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write(full_content)

    # Summarize web search results
    # result_summarized = call_nvidia_llm("meta/llama-4-scout-17b-16e-instruct", prompt, max_tokens=1024)
    result_summarized = call_google_llm("gemini-2.0-flash-lite", prompt, max_tokens=1024)

    # Save summarized web search results
    SUMMARIZED_WEB_SEARCH_RESULTS_FILE = SCRAPPER_DIR / "summarized_web_search_results.txt"
    with open(SUMMARIZED_WEB_SEARCH_RESULTS_FILE, "w", encoding="utf-8") as f:
        f.write(result_summarized)

    return result_summarized


if __name__ == "__main__":
    search("Gustavo Bellanda")
